# =============================================================================
# OCR Provenance MCP System - Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your values:
#   cp .env.example .env

# -----------------------------------------------------------------------------
# API KEYS (Required)
# -----------------------------------------------------------------------------
# Get your Datalab API key from https://www.datalab.to
DATALAB_API_KEY=your_datalab_key_here

# Get your Gemini API key from https://aistudio.google.com/
# Required for: entity extraction, VLM image descriptions, QA, relationship classification
GEMINI_API_KEY=your_gemini_key_here

# -----------------------------------------------------------------------------
# DATALAB API CONFIGURATION
# -----------------------------------------------------------------------------
# Datalab API base URL
DATALAB_BASE_URL=https://www.datalab.to/api/v1

# OCR processing mode: fast | balanced | accurate
DATALAB_DEFAULT_MODE=accurate

# Maximum concurrent API requests (1-10)
DATALAB_MAX_CONCURRENT=3

# Request timeout in milliseconds (5 minutes default)
DATALAB_TIMEOUT=300000

# -----------------------------------------------------------------------------
# GEMINI API CONFIGURATION (optional - all have sensible defaults)
# -----------------------------------------------------------------------------
# Model to use: gemini-2.0-flash | gemini-3-flash-preview | gemini-2.5-pro
GEMINI_MODEL=gemini-2.0-flash

# Subscription tier (affects rate limiting): free | payAsYouGo | enterprise
GEMINI_TIER=payAsYouGo

# Maximum output tokens per Gemini call (1-65536)
GEMINI_MAX_OUTPUT_TOKENS=8192

# Temperature for generation (0.0 = deterministic, 2.0 = creative)
GEMINI_TEMPERATURE=0.0

# Image resolution for VLM: MEDIA_RESOLUTION_HIGH | MEDIA_RESOLUTION_LOW
GEMINI_MEDIA_RESOLUTION=MEDIA_RESOLUTION_HIGH

# Max parallel entity extraction segments (higher = faster but more API calls)
GEMINI_PARALLEL_SEGMENTS=3

# -----------------------------------------------------------------------------
# EMBEDDING MODEL CONFIGURATION
# -----------------------------------------------------------------------------
# Local embedding model path (download nomic-embed-text-v1.5 from HuggingFace)
# See README.md for download instructions
EMBEDDING_MODEL=./models/nomic-embed-text-v1.5

# Embedding vector dimensions (768 for nomic-embed-text-v1.5)
EMBEDDING_DIMENSIONS=768

# Batch size for embedding generation
EMBEDDING_BATCH_SIZE=512

# Embedding device: cuda | mps (Apple Silicon) | cpu
EMBEDDING_DEVICE=cuda
EMBEDDING_USE_GPU=true
EMBEDDING_TRUST_REMOTE_CODE=true

# -----------------------------------------------------------------------------
# GPU CONFIGURATION
# -----------------------------------------------------------------------------
# GPU device identifier: cuda:0 | mps (Apple Silicon)
GPU_DEVICE=cuda:0

# Data type for GPU inference: float16 | float32 | bfloat16
GPU_DTYPE=float16

# Enable torch.compile for model optimization
GPU_COMPILE_MODEL=true

# Maximum GPU memory fraction to use (0.0 - 1.0)
GPU_MEMORY_FRACTION=0.9

# -----------------------------------------------------------------------------
# CUDA / PYTORCH ENVIRONMENT
# -----------------------------------------------------------------------------
# Force CUDA device visibility
CUDA_VISIBLE_DEVICES=0

# PyTorch CUDA settings
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# Set to false for Apple Silicon (MPS) or CPU-only setups
FORCE_GPU=true
NO_CPU_FALLBACK=true

# -----------------------------------------------------------------------------
# TEXT CHUNKING CONFIGURATION
# -----------------------------------------------------------------------------
# Maximum characters per chunk
CHUNKING_SIZE=2000

# Overlap percentage between chunks (0-50)
CHUNKING_OVERLAP_PERCENT=10

# -----------------------------------------------------------------------------
# STORAGE CONFIGURATION
# -----------------------------------------------------------------------------
# Path to store SQLite databases with vector extensions
STORAGE_DATABASES_PATH=~/.ocr-provenance/databases/

# Current active database name (optional)
STORAGE_CURRENT_DATABASE=

# -----------------------------------------------------------------------------
# PROVENANCE TRACKING CONFIGURATION
# -----------------------------------------------------------------------------
# Hash algorithm for document integrity: sha256 | sha512 | blake2b
PROVENANCE_HASH_ALGORITHM=sha256

# Export format for provenance data: json | xml
PROVENANCE_EXPORT_FORMAT=json

# -----------------------------------------------------------------------------
# MCP SERVER CONFIGURATION
# -----------------------------------------------------------------------------
# Server name for MCP protocol
MCP_SERVER_NAME=ocr-provenance

# Server version
MCP_SERVER_VERSION=1.0.0

# Enable debug logging
MCP_DEBUG=false

# Log level: debug | info | warn | error
LOG_LEVEL=info
