# =============================================================================
# OCR Provenance MCP System - Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your values:
#   cp .env.example .env

# -----------------------------------------------------------------------------
# API KEYS (Required)
# -----------------------------------------------------------------------------
# Get your Datalab API key from https://www.datalab.to
DATALAB_API_KEY=your_datalab_key_here

# Get your Gemini API key from https://aistudio.google.com/
# Required for: entity extraction, VLM image descriptions, QA, relationship classification
GEMINI_API_KEY=your_gemini_key_here

# -----------------------------------------------------------------------------
# DATALAB API CONFIGURATION (optional - all have sensible defaults)
# -----------------------------------------------------------------------------
# Maximum concurrent API requests (1-10)
DATALAB_MAX_CONCURRENT=3

# Request timeout in milliseconds (must be > Python worker timeout of 300s + buffer)
DATALAB_TIMEOUT=330000

# -----------------------------------------------------------------------------
# GEMINI API CONFIGURATION (optional - all have sensible defaults)
# -----------------------------------------------------------------------------
# Model to use (only gemini-3-flash-preview is supported)
GEMINI_MODEL=gemini-3-flash-preview

# Maximum output tokens per Gemini call (1-65536)
GEMINI_MAX_OUTPUT_TOKENS=8192

# Temperature for generation (0.0 = deterministic, 2.0 = creative)
GEMINI_TEMPERATURE=0.0

# Image resolution for VLM: MEDIA_RESOLUTION_HIGH | MEDIA_RESOLUTION_LOW
GEMINI_MEDIA_RESOLUTION=MEDIA_RESOLUTION_HIGH

# Max parallel entity extraction segments (higher = faster but more API calls)
GEMINI_PARALLEL_SEGMENTS=5

# -----------------------------------------------------------------------------
# EMBEDDING DEVICE (optional)
# -----------------------------------------------------------------------------
# Embedding device: auto | cuda | cuda:0 | mps (Apple Silicon) | cpu
# "auto" detects the best available: CUDA > MPS > CPU
EMBEDDING_DEVICE=auto

# -----------------------------------------------------------------------------
# PYTORCH / CUDA ENVIRONMENT (optional)
# -----------------------------------------------------------------------------
# These are consumed by PyTorch runtime, not the application code directly.
# Uncomment to override PyTorch defaults.
#
# CUDA_VISIBLE_DEVICES=0
# PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
